{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to be stored in a class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Normalized Gini Coefficient (gini_normalizado)\n",
    "We'll convert the gini_normalizado function to work with PySpark DataFrames. Since the Gini coefficient calculation involves sorting and cumulative sums, we'll use window functions in PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import col, sum as spark_sum, desc, asc, row_number\n",
    "\n",
    "def gini_normalizado(spark_df, actual_col, pred_col):\n",
    "    \"\"\"\n",
    "    Calculates the normalized Gini coefficient using PySpark DataFrame.\n",
    "\n",
    "    :param spark_df: PySpark DataFrame containing actual and predicted values.\n",
    "    :param actual_col: Name of the column with actual values.\n",
    "    :param pred_col: Name of the column with predicted values.\n",
    "    :return: Normalized Gini coefficient.\n",
    "    \"\"\"\n",
    "    # Total number of records\n",
    "    n = spark_df.count()\n",
    "    \n",
    "    # Create a window for ordering by predicted values\n",
    "    window = Window.orderBy(desc(pred_col), asc(\"row_idx\"))\n",
    "    \n",
    "    # Add a row index to maintain the original order\n",
    "    df = spark_df.select(actual_col, pred_col).withColumn(\"row_idx\", row_number().over(Window.orderBy(lit(1))))\n",
    "    \n",
    "    # Calculate cumulative sum of actual values\n",
    "    df = df.withColumn(\"cumulative_actual\", spark_sum(col(actual_col)).over(window))\n",
    "    \n",
    "    # Calculate total actual values\n",
    "    total_actual = df.agg(spark_sum(actual_col).alias(\"total_actual\")).collect()[0][\"total_actual\"]\n",
    "    \n",
    "    # Calculate Gini numerator\n",
    "    df = df.withColumn(\"lorentz\", col(\"cumulative_actual\") / total_actual)\n",
    "    gini_sum = df.agg(spark_sum(\"lorentz\").alias(\"gini_sum\")).collect()[0][\"gini_sum\"]\n",
    "    \n",
    "    # Adjust Gini sum\n",
    "    gini_sum -= (n + 1) / 2.0\n",
    "    \n",
    "    # Calculate normalized Gini\n",
    "    normalized_gini = gini_sum / n\n",
    "    \n",
    "    return normalized_gini\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'df' is your DataFrame with 'actual' and 'predicted' columns\n",
    "gini = gini_normalizado(df, 'actual', 'predicted')\n",
    "print(f\"Normalized Gini Coefficient: {gini}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Generate Metadata (pod_academy_generate_metadata)\n",
    "This function generates metadata about the DataFrame columns, such as null counts, percentages, and cardinality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, sum, when\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# # Contar valores nulos em cada coluna\n",
    "# valores_nulos = df_dm.select([sum(when(col(c).isNull(), 1).otherwise(0)).alias(c) for c in df_dm.columns])\n",
    "# valores_nulos.show()\n",
    "# Contar valores nulos em cada coluna e coletar os resultados\n",
    "\n",
    "# Contar o número total de linhas no DataFrame\n",
    "total_linhas = df_dm.count()\n",
    "\n",
    "# Contar valores nulos em cada coluna e coletar os resultados\n",
    "valores_nulos = df_dm.select([\n",
    "    sum(when(col(c).isNull(), 1).otherwise(0)).alias(c) for c in df_dm.columns\n",
    "])\n",
    "\n",
    "# Coletar os resultados como um dicionário\n",
    "nulos_dict = valores_nulos.collect()[0].asDict()\n",
    "\n",
    "# Converter o dicionário em uma lista de Rows, adicionando o percentual de valores nulos\n",
    "nulos_lista = [\n",
    "    Row(\n",
    "        Coluna=k, \n",
    "        Valores_Nulos=v, \n",
    "        Porcentagem_Nulos=round((v / total_linhas) * 100, 2)\n",
    "    ) for k, v in nulos_dict.items()\n",
    "]\n",
    "\n",
    "# Criar um DataFrame Spark a partir da lista\n",
    "valores_nulos_df = spark.createDataFrame(nulos_lista)\n",
    "\n",
    "# Exibir os valores nulos e percentuais por coluna\n",
    "valores_nulos_df.show(valores_nulos_df.count(), truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df = pod_academy_generate_metadata(df)\n",
    "metadata_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Generate Metadata with Usage (pod_academy_generate_metadata with IDs and Targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, isnan, when, countDistinct\n",
    "\n",
    "def pod_academy_generate_metadata(spark_df, ids, targets, orderby='PC_NULOS'):\n",
    "    \"\"\"\n",
    "    Returns a table with descriptive information about a PySpark DataFrame.\n",
    "\n",
    "    :param spark_df: PySpark DataFrame to describe.\n",
    "    :param ids: List of columns that are identifiers.\n",
    "    :param targets: List of columns that are target variables.\n",
    "    :param orderby: Column name to order by.\n",
    "    :return: PySpark DataFrame with descriptive information.\n",
    "    \"\"\"\n",
    "    total_rows = spark_df.count()\n",
    "    summary_list = []\n",
    "    \n",
    "    for column in spark_df.columns:\n",
    "        uso_feature = 'ID' if column in ids else 'Target' if column in targets else 'Explicativa'\n",
    "        qt_nulos = spark_df.filter(col(column).isNull() | isnan(col(column))).count()\n",
    "        pc_nulos = (qt_nulos / total_rows) * 100\n",
    "        cardinalidade = spark_df.select(countDistinct(col(column))).collect()[0][0]\n",
    "        tipo_feature = str(spark_df.schema[column].dataType)\n",
    "        \n",
    "        summary_list.append((column, uso_feature, qt_nulos, round(pc_nulos, 2), cardinalidade, tipo_feature))\n",
    "    \n",
    "    summary_df = spark.createDataFrame(summary_list, ['FEATURE', 'USO_FEATURE', 'QT_NULOS', 'PC_NULOS', 'CARDINALIDADE', 'TIPO_FEATURE'])\n",
    "    \n",
    "    # Sort by specified column\n",
    "    summary_df = summary_df.orderBy(col(orderby).desc())\n",
    "    \n",
    "    return summary_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = ['id_column']\n",
    "targets = ['target_column']\n",
    "summary_df = pod_academy_generate_metadata(df, ids, targets, orderby='PC_NULOS')\n",
    "summary_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Custom Fill NA (pod_custom_fillna)\n",
    "This function replaces -1 with null and fills missing values for numerical and categorical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, mean as spark_mean, lit\n",
    "from pyspark.sql.types import NumericType, StringType\n",
    "\n",
    "def pod_custom_fillna(spark_df):\n",
    "    \"\"\"\n",
    "    Replaces missing values in the DataFrame.\n",
    "\n",
    "    - Replaces -1 with null.\n",
    "    - Fills numerical columns with the mean.\n",
    "    - Fills categorical columns with 'POD_VERIFICAR'.\n",
    "\n",
    "    :param spark_df: PySpark DataFrame to process.\n",
    "    :return: Tuple of (Processed DataFrame, Dictionary of means).\n",
    "    \"\"\"\n",
    "    # Replace -1 with null\n",
    "    df = spark_df.replace(-1, None)\n",
    "    \n",
    "    numerical_cols = [field.name for field in df.schema.fields if isinstance(field.dataType, NumericType)]\n",
    "    categorical_cols = [field.name for field in df.schema.fields if isinstance(field.dataType, StringType)]\n",
    "    \n",
    "    means = {}\n",
    "    \n",
    "    # Compute means for numerical columns\n",
    "    for col_name in numerical_cols:\n",
    "        mean_value = df.select(spark_mean(col(col_name))).first()[0]\n",
    "        means[col_name] = mean_value\n",
    "        df = df.withColumn(col_name, when(col(col_name).isNull(), lit(mean_value)).otherwise(col(col_name)))\n",
    "    \n",
    "    # Fill categorical columns with 'POD_VERIFICAR'\n",
    "    df = df.fillna('POD_VERIFICAR', subset=categorical_cols)\n",
    "    \n",
    "    return df, means\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filled, means = pod_custom_fillna(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Custom Fill NA for Production (pod_custom_fillna_prod)\n",
    "This function fills missing values using provided means (useful in production environments)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, lit\n",
    "from pyspark.sql.types import NumericType, StringType\n",
    "\n",
    "def pod_custom_fillna_prod(spark_df, means):\n",
    "    \"\"\"\n",
    "    Fills missing values in the DataFrame using provided means.\n",
    "\n",
    "    :param spark_df: PySpark DataFrame to process.\n",
    "    :param means: Dictionary of means for numerical columns.\n",
    "    :return: Processed DataFrame.\n",
    "    \"\"\"\n",
    "    # Replace -1 with null\n",
    "    df = spark_df.replace(-1, None)\n",
    "    \n",
    "    numerical_cols = [field.name for field in df.schema.fields if isinstance(field.dataType, NumericType)]\n",
    "    categorical_cols = [field.name for field in df.schema.fields if isinstance(field.dataType, StringType)]\n",
    "    \n",
    "    # Fill numerical columns with provided means\n",
    "    for col_name in numerical_cols:\n",
    "        mean_value = means.get(col_name)\n",
    "        if mean_value is not None:\n",
    "            df = df.withColumn(col_name, when(col(col_name).isNull(), lit(mean_value)).otherwise(col(col_name)))\n",
    "    \n",
    "    # Fill categorical columns with 'POD_VERIFICAR'\n",
    "    df = df.fillna('POD_VERIFICAR', subset=categorical_cols)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'means' is the dictionary obtained from 'pod_custom_fillna'\n",
    "df_prod_filled = pod_custom_fillna_prod(df_new, means)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Evaluate Model (avaliar_modelo)\n",
    "Converting the avaliar_modelo function is a bit more complex because PySpark does not directly support plotting with Matplotlib. However, we can perform the calculations in PySpark and collect the results for plotting.\n",
    "\n",
    "Important Notes:\n",
    "\n",
    "Since plotting is not natively supported in PySpark, we collect the necessary data to the driver node for plotting.\n",
    "Be cautious when collecting data; ensure that the datasets are small enough to fit into memory.\n",
    "The plotting code would remain largely the same as your original function, using the collected data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.classification import LogisticRegressionModel\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def avaliar_modelo(df_train, df_test, features_col, label_col, modelo, nm_modelo):\n",
    "    \"\"\"\n",
    "    Evaluates the model by computing various metrics and plotting graphs.\n",
    "\n",
    "    :param df_train: PySpark DataFrame for training data.\n",
    "    :param df_test: PySpark DataFrame for test data.\n",
    "    :param features_col: Name of the features column (assembled vector).\n",
    "    :param label_col: Name of the label column.\n",
    "    :param modelo: Trained PySpark model.\n",
    "    :param nm_modelo: Name of the model.\n",
    "    \"\"\"\n",
    "    # Predict on training and test data\n",
    "    predictions_train = modelo.transform(df_train)\n",
    "    predictions_test = modelo.transform(df_test)\n",
    "    \n",
    "    # Convert predictions to Pandas DataFrames\n",
    "    y_train = predictions_train.select(label_col).toPandas()\n",
    "    y_pred_train = predictions_train.select('prediction').toPandas()\n",
    "    y_score_train = predictions_train.select('probability').rdd.map(lambda x: float(x['probability'][1])).collect()\n",
    "    \n",
    "    y_test = predictions_test.select(label_col).toPandas()\n",
    "    y_pred_test = predictions_test.select('prediction').toPandas()\n",
    "    y_score_test = predictions_test.select('probability').rdd.map(lambda x: float(x['probability'][1])).collect()\n",
    "    \n",
    "    # Import necessary sklearn metrics\n",
    "    from sklearn.metrics import confusion_matrix, roc_curve, precision_recall_curve, roc_auc_score\n",
    "    \n",
    "    # Proceed to plotting as in your original function, using the collected data\n",
    "    # ... (Include your plotting code here, using y_train, y_pred_train, y_score_train, etc.)\n",
    "    # For example:\n",
    "\n",
    "    # Confusion Matrix - Training\n",
    "    cm_train = confusion_matrix(y_train, y_pred_train)\n",
    "    # Similarly for test data and other metrics\n",
    "    \n",
    "    # Create the plots as before using Matplotlib\n",
    "    # Note: Ensure that the data size is manageable for collecting to the driver\n",
    "\n",
    "    # (Plotting code should replicate the functionality of your original function)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Calculate KS Statistic (calcular_ks_statistic)\n",
    "We'll use PySpark window functions to compute the KS statistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, sum as spark_sum, desc, lit\n",
    "from pyspark.sql import Window\n",
    "\n",
    "\n",
    "def calcular_ks_statistic(spark_df, label_col, score_col):\n",
    "    \"\"\"\n",
    "    Calculates the KS statistic using a PySpark DataFrame.\n",
    "\n",
    "    :param spark_df: PySpark DataFrame containing actual labels and scores.\n",
    "    :param label_col: Name of the label column.\n",
    "    :param score_col: Name of the score (prediction probability) column.\n",
    "    :return: KS statistic.\n",
    "    \"\"\"\n",
    "    \n",
    "    total_events = spark_df.filter(col(label_col) == 1).count()\n",
    "    total_non_events = spark_df.filter(col(label_col) == 0).count()\n",
    "    \n",
    "    # Create cumulative counts\n",
    "    window = Window.orderBy(desc(score_col)).rowsBetween(Window.unboundedPreceding, 0)\n",
    "    \n",
    "    df = spark_df.withColumn('event', when(col(label_col) == 1, 1).otherwise(0))\n",
    "    df = df.withColumn('non_event', when(col(label_col) == 0, 1).otherwise(0))\n",
    "    \n",
    "    df = df.withColumn('cum_event', spark_sum('event').over(window))\n",
    "    df = df.withColumn('cum_non_event', spark_sum('non_event').over(window))\n",
    "    \n",
    "    df = df.withColumn('cum_event_rate', col('cum_event') / total_events)\n",
    "    df = df.withColumn('cum_non_event_rate', col('cum_non_event') / total_non_events)\n",
    "    \n",
    "    # Calculate KS statistic\n",
    "    df = df.withColumn('ks', abs(col('cum_event_rate') - col('cum_non_event_rate')))\n",
    "    ks_statistic = df.agg({'ks': 'max'}).collect()[0][0]\n",
    "    \n",
    "    return ks_statistic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'df' has 'label' and 'score' columns\n",
    "ks_stat = calcular_ks_statistic(df, 'label', 'score')\n",
    "print(f\"KS Statistic: {ks_stat}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hackthon_dm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
